"""
Finds the first link in a wikipedia page by scraping the html.
"""

from urllib.parse import unquote
import multiprocessing
from bs4 import BeautifulSoup
import wikipedia
from obtain_wiki_data import random_page


def find_first_link(depth, id_num, return_dict, wikipedia_page=random_page()):
    """
    From a random wikipedia page, the first wikipedia link on that page is
    chosen. The first link that is embedded on that link's page is then
    chosen and so on and so forth for depth times or until the links are
    looped or nonexistant. The "path" created by this process will be added to
    return_dict with a key of id_num.

    Args:
        depth: an integer of the maximum number of times the first link will
            traced.

        id_num: the identification integer for the current process (used to
            synchronize multiprocessing workers).

        return_dict: the dictionary in which the results of the process will
            be added.

        wikipedia_page (optional): a string of the wikipedia page you want to
            start with. If left blank, the page will be random.

    Returns:
        None.
    """

    def scan_links(html_text):
        """
        A helper function to determine what the first link actually is.

        Args:
            html_text: a string of html to scan

        Returns:
            A valid wikipedia page generated by the first link on another page.
            If no valid wikipedia page is found, None type is returned.
        """
        for paragraph in html_text:
            for tag in paragraph.find_all("a"):
                link = tag.get("href", "")
                if (
                    link[:6] == "/wiki/"
                    and link[6:11] != "Help:"
                    and link[6:16] != "Wikipedia:"
                ):
                    try:
                        return wikipedia.page(unquote(link[6:]), auto_suggest=False)
                    except wikipedia.exceptions.PageError:
                        continue
                    except KeyError:
                        return None
        return None

    pages = []
    page_titles = []
    pages.append(wikipedia_page)
    while depth > 0 and wikipedia_page is not None and wikipedia_page not in pages[:-1]:
        soup = BeautifulSoup(wikipedia_page.html(), "html.parser")
        body_text = soup.find_all("p")
        wikipedia_page = scan_links(body_text)
        pages.append(wikipedia_page)
        depth -= 1

    for page in pages:
        if page is not None:
            page_titles.append(page.title)
    return_dict[id_num] = page_titles


def follow_first_links(threads=12, depth=50):
    """
    Creates a multiprocessing process to obtain the path of first_links
    starting from a random wikipedia page.

    Args:
        threads (optional): the integer number of workers to be active.
        Default is 12.

        depth (optional): the integer number of the maximum length of the path
        of first links. Default is 50.

    Returns:
        A list of lists where each interior list is a list of the links from
        a random wikipedia page.
    """
    manager = multiprocessing.Manager()
    return_dict = manager.dict()
    jobs = []
    for i in range(threads):
        process = multiprocessing.Process(
            target=find_first_link, args=(depth, i, return_dict, random_page())
        )
        jobs.append(process)
        process.start()

    for proc in jobs:
        proc.join()
    return return_dict.values()
