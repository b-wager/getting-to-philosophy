"""
Functions to graph data.
"""

from collections import Counter
import multiprocessing
import numpy as np
import matplotlib.pyplot as plt
import pygraphviz as pgv


def generate_edges(page_titles):
    """
    Helper function that takes page_titles and returns a list of tuple pairs
    to connect linked pages.

    Args:
        page_titles: a list of strings of the titles of linked wikipedia pages.

    Returns:
        A list of tuples that contain pairs of pages that are linked together.
    """
    edges = []
    for i in range(1, len(page_titles)):
        edges.append((page_titles[i - 1], page_titles[i]))
    return edges


def get_paths(threads, depth, path_function):
    """
    Uses path_function to trace paths by navigating links.

    Args:
        threads: how many processes, and by extension threads,
                to run simeltaneously.
        depth: how many links to trace on each worker
    Returns:
        An list of ordered lists that represent the paths
        of workers each generated by path_function.
    """
    manager = multiprocessing.Manager()
    return_dict = manager.dict()
    jobs = []
    for id_num in range(threads):
        processes = multiprocessing.Process(
            target=path_function, args=(depth, id_num, return_dict)
        )
        jobs.append(processes)
        processes.start()

    for proc in jobs:
        proc.join()
    return return_dict.values()


def graph_network(page_titles_list, filename):
    """
    Creates a network graph and saves it to "file.png".

    Args:
        page_titles_list: a list of lists of strings where the embedded lists
        contain the path data for the title of wikipedia pages.
        filename: The name (or path) of where to save the data (string)

    Returns:
        None.
    """
    options = {
        "node_color": "black",
        "node_size": 10,
        "width": 1,
    }
    page_network = pgv.AGraph()
    nodes = []
    edges = []
    for page_titles in page_titles_list:
        nodes = nodes + page_titles
        edges = edges + generate_edges(page_titles)
    nodes = [*set(nodes)]
    edges = [*set(edges)]
    page_network.add_nodes_from(nodes)
    page_network.add_edges_from(edges, len=2)
    page_network.layout()
    page_network.draw(filename)
    plt.show()


def plot_frequency(all_links, title, flat=False, num_graphed=20):
    """
    Creates a frequency bar graph.

    Args:
        all_links: a list of lists of strings or a list of strings where the
        strings are titles of wikipedia pages.

        title: the title of the graph.

        flat (optional): If True, all_links is a list of strings. If False,
        all_links is a list of lists of strings. Default is False.

        num_graphed (optional): the number of bars to include on the graph.
        Default is 20.
    """
    if flat:
        all_links_flat = all_links
    else:
        all_links_flat = []
        for links in all_links:
            for link in links:
                all_links_flat.append(link)

    counts = Counter(all_links_flat)
    (labels, values) = zip(*counts.items())
    # sort your values in descending order
    ind_sort = np.argsort(values)[::-1]
    # rearrange your data
    labels = np.array(labels)[ind_sort][num_graphed - 1 :: -1]
    values = np.array(values)[ind_sort][num_graphed - 1 :: -1]
    indexes = np.arange(len(labels))
    plt.figure(figsize=(10, num_graphed * 0.4))
    plt.barh(indexes, values)
    # add labels
    plt.yticks(indexes, labels)
    plt.title(title)
    plt.xlabel("Number of Occurrences")
    plt.ylabel("Linked Page Name")
    plt.show()


def graph_philosophy_depth(page_titles_list):
    """
    Creates a histogram of the path length it takes to get from the beginning
    of a wikipedia page path to "Philosophy."

    Args:
        page_titles_list: a list of lists of strings where each embedded list
        represents the path between wikipedia pages.

    Returns:
        a list of the length of the path to Philosophy. A value of -2 means that
        the page was unable to reach Philosophy.
    """
    depths = []
    for path in page_titles_list:
        try:
            depths.append(path.index("Philosophy"))
        except ValueError:
            depths.append(-2)
    plt.hist(depths, bins=20, rwidth=0.7)
    plt.title("Distribution of How Long the Path to Philosophy Is")
    plt.xlabel("Length of Path")
    plt.ylabel("Number of Occurances")
    plt.show()

    return depths


def graph_multiprocessing_failures():
    """
    Creates a line graph depicting the number of attempted workers at once
    compared to the number of errors thrown. Just for funsies! Please don't
    grade this it is not part of the project. It's fun to look at tho.

    Args:
        None.

    Returns:
        Joe Mama.
    """
    # keys are number of attempted requests and values are number of successes
    erroring_out = {
        1: 1,
        2: 2,
        4: 4,
        8: 8,
        16: 16,
        32: 32,
        64: 64,
        128: 128,
        256: 256,
        512: 512,
        1024: 625,
        2048: 474,
        4096: 712,
        8192: 1506,
    }
    # same as before but with values being number of errors instead
    errors = {}
    for value in erroring_out.items():
        errors[value[0]] = value[0] - value[1]

    plt.plot(errors.keys(), errors.values(), marker="$ðŸ˜€$", ms=25, mec="k")
    plt.xscale("log")
    plt.xlabel("Attempted Requests")
    plt.ylabel("Failed Requests")
    plt.title("Attempted vs. Failed Requests of Wikipedia Pages")
    plt.show()
    return "Joe Mama"
